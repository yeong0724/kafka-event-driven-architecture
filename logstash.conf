# Kafka Consumer 정의
input {
    kafka {
        bootstrap_servers => "kafka1:19092,kafka2:19092,kafka3:19092"
        topics => ["post.original"]
        group_id => "logstash-v1"
        consumer_threads => 3
        auto_offset_reset => "earliest"
        codec => "json"
        type => "kafka"
    }
}

# Log Data 전처리 단계 정의
# filter {
#     # grok: 비정형화 데이터를 정형회된 정규표현식 처럼 파싱처리 하는 것
#     grok {
#         match => { "message" => "%{IPORHOST:client_ip} - - \[%{HTTPDATE:logged_at}\] \"%{WORD:http_method} %{URIPATH:request} HTTP/%{NUMBER:http_version}\" %{NUMBER:response_code} %{NUMBER:bytes} %{NUMBER:duration} \"%{DATA:referrer}\" \"%{DATA:user_agent}\"" }
#     }
#     if "_grokparsefailure" in [tags] {
#         drop {}
#     }
#     date {
#         match => [ "logged_at", "dd/MMM/yyyy:HH:mm:ss Z" ]
#         target => "logged_at"
#     }
#     mutate {
#         remove_field => [ "message" ]
#     }
# }

output {
    elasticsearch {
        hosts => ["elasticsearch:9200"]
        index => "post-%{+YYYY-MM-dd}"
    }
}